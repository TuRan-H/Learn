{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa79c408",
   "metadata": {},
   "source": [
    "# 如何使用一个Runnable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "221f48d4",
   "metadata": {},
   "source": [
    "`RunnableLambda` 也是一个经典的Runnable类型, 输入参数是一个函数, 使用 `RunableLambda` 将该函数封装为一个Runnable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1a6cfd05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "add_one_runnable = RunnableLambda(lambda x: x + 1)\n",
    "result = add_one_runnable.invoke(5)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c10a599",
   "metadata": {},
   "source": [
    "# 使用Runnable编排工作流"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1734ba4",
   "metadata": {},
   "source": [
    "使用langchain提供的LCEL (langchain expression language) 来实现工作流\n",
    "\n",
    "具体来说就是使用类似于Linux中pipe的语法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1dfd10c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'中国的首都是北京。北京是中国的政治、文化和国际交流中心，具有重要的地位和丰富的历史文化遗产。'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_community.llms.tongyi import Tongyi\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "input_data = {\"user_name\": \"张三\", \"question\": \"中国的首都是哪里？\", \"request_id\": \"xyz-123\"}\n",
    "\n",
    "get_question_runnable = RunnableLambda(func=lambda x: x.get(\"question\", \"no question\"))\n",
    "\n",
    "llm = Tongyi(name=\"qwen-plus\")\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=\"{question}\")\n",
    "\n",
    "chain = get_question_runnable | prompt | llm | StrOutputParser()\n",
    "chain.invoke(input_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
